{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4SMmTviVm3s"
      },
      "source": [
        "# Module 5 Homework\n",
        "In this homework we'll put what we learned about Spark in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2GFuxGCUjHU",
        "outputId": "57429506-aaad-461b-d915-3f68bf20fa6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: pyspark[connect,pandas_on_spark,sql] in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect,pandas_on_spark,sql]) (0.10.9.7)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect,pandas_on_spark,sql]) (2.2.2)\n",
            "Requirement already satisfied: pyarrow>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect,pandas_on_spark,sql]) (18.1.0)\n",
            "Requirement already satisfied: numpy<2,>=1.15 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect,pandas_on_spark,sql]) (1.26.4)\n",
            "Requirement already satisfied: grpcio>=1.56.0 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect,pandas_on_spark,sql]) (1.70.0)\n",
            "Requirement already satisfied: grpcio-status>=1.56.0 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect,pandas_on_spark,sql]) (1.62.3)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from pyspark[connect,pandas_on_spark,sql]) (1.68.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from googleapis-common-protos>=1.56.4->pyspark[connect,pandas_on_spark,sql]) (4.25.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->pyspark[connect,pandas_on_spark,sql]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->pyspark[connect,pandas_on_spark,sql]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->pyspark[connect,pandas_on_spark,sql]) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[connect,pandas_on_spark,sql]) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark and ngrok\n",
        "!pip install pyspark[sql,pandas_on_spark,connect] pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsEY3ncgWMra"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "P7f7KCmGV-RV"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyngrok import ngrok\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mBJF0d1rWRhQ"
      },
      "outputs": [],
      "source": [
        "# --- Securely Set ngrok Auth Token ---\n",
        "# You can store your auth token in a Colab secret\n",
        "# To do this, go to the left sidebar, click the \"Secrets\" tab (key icon),\n",
        "# and add a secret named \"NGROK_AUTH_TOKEN\" with your token as the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mqiBfDjPaCVl"
      },
      "outputs": [],
      "source": [
        "def setup_spark_ngrok():\n",
        "    \"\"\"Sets up SparkSession and ngrok tunnel, returning both objects.\"\"\"\n",
        "    try:\n",
        "        ngrok_auth_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "    except KeyError:\n",
        "        print(\"ngrok auth token not found. Please add it as a Colab secret.\")\n",
        "        return None, None\n",
        "\n",
        "    if ngrok_auth_token:\n",
        "        ngrok.set_auth_token(ngrok_auth_token)\n",
        "        spark = SparkSession.builder.appName(\"ColabSparkUI\").getOrCreate()\n",
        "        ngrok_tunnel = ngrok.connect(4040)\n",
        "        print(\"Spark UI Public URL:\", ngrok_tunnel.public_url)\n",
        "        return spark, ngrok_tunnel\n",
        "    else:\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODiIRDsiZBtS",
        "outputId": "75950dd9-b4ba-4876-de48-091261b2c32e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-03-06T17:06:45+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark UI Public URL: https://57cd-34-125-55-49.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "spark, ngrok_tunnel = setup_spark_ngrok()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psJyie0EZBqO",
        "outputId": "c32c157e-cd52-41f4-ba61-64333fbd29fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SparkSession and ngrok tunnel are ready.\n"
          ]
        }
      ],
      "source": [
        "if spark and ngrok_tunnel:\n",
        "    # You can now use 'spark' and 'ngrok_tunnel' in subsequent cells\n",
        "    print(\"SparkSession and ngrok tunnel are ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abYkxeI6iZVz"
      },
      "source": [
        "## Question 1: Install Spark and PySpark\n",
        "\n",
        "- Install Spark\n",
        "- Run PySpark\n",
        "- Create a local spark session\n",
        "- Execute spark.version.\n",
        "\n",
        "What's the output?\n",
        "\n",
        "> [!NOTE]\n",
        "> To install PySpark follow this [guide](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/pyspark.md)\n",
        "\n",
        "> Please note in this notebook I am using Google Colab rather than local Spark installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXWyoE2QZBnL",
        "outputId": "ce229cc4-c6b2-43f8-95fb-8977468180a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark Version: 3.5.5\n"
          ]
        }
      ],
      "source": [
        "# --- Question 1: Install Spark and PySpark ---\n",
        "print(\"Spark Version:\", spark.version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wznQHIbvZBkK",
        "outputId": "5d5d16e0-f63a-4d41-8c7d-383300ad50c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-06 17:11:01--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
            "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.239.38.147, 18.239.38.181, 18.239.38.83, ...\n",
            "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.239.38.147|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64346071 (61M) [binary/octet-stream]\n",
            "Saving to: ‘yellow_tripdata_2024-10.parquet’\n",
            "\n",
            "yellow_tripdata_202 100%[===================>]  61.36M  18.6MB/s    in 3.8s    \n",
            "\n",
            "2025-03-06 17:11:05 (16.3 MB/s) - ‘yellow_tripdata_2024-10.parquet’ saved [64346071/64346071]\n",
            "\n",
            "--2025-03-06 17:11:05--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
            "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.239.38.147, 18.239.38.181, 18.239.38.83, ...\n",
            "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.239.38.147|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12331 (12K) [text/csv]\n",
            "Saving to: ‘taxi_zone_lookup.csv’\n",
            "\n",
            "taxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-03-06 17:11:06 (1.05 MB/s) - ‘taxi_zone_lookup.csv’ saved [12331/12331]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# --- Download Data ---\n",
        "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
        "!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNCcH-XwcEuR"
      },
      "source": [
        "## Question 2: Yellow October 2024\n",
        "\n",
        "Read the October 2024 Yellow into a Spark Dataframe.\n",
        "\n",
        "Repartition the Dataframe to 4 partitions and save it to parquet.\n",
        "\n",
        "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "kLMSdHzacCIi"
      },
      "outputs": [],
      "source": [
        "df = spark.read.parquet(\"yellow_tripdata_2024-10.parquet\")\n",
        "df_repartitioned = df.repartition(4)\n",
        "df_repartitioned.write.parquet(\"yellow_tripdata_2024-10-repartitioned.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2XNsxoxcUof",
        "outputId": "ab4591c5-bcf7-4b22-b1e6-c32bc894c2b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Parquet file size: 23.042235136032104 MB\n"
          ]
        }
      ],
      "source": [
        " # Calculate average file size\n",
        "import os\n",
        "parquet_files = [f for f in os.listdir(\"yellow_tripdata_2024-10-repartitioned.parquet\") if f.endswith(\".parquet\")]\n",
        "total_size = 0\n",
        "for file in parquet_files:\n",
        "    total_size += os.path.getsize(os.path.join(\"yellow_tripdata_2024-10-repartitioned.parquet\", file))\n",
        "average_size_mb = (total_size / len(parquet_files)) / (1024 * 1024)\n",
        "print(f\"Average Parquet file size: {average_size_mb} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6BFcVMJfBDp"
      },
      "source": [
        "I will also both Spark Sql and Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NFqAbt8ne_jV"
      },
      "outputs": [],
      "source": [
        "# Using Spark SQL\n",
        "# Register the DataFrame as a temporary view\n",
        "df.createOrReplaceTempView(\"trip_data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX8pkcVHctZc"
      },
      "source": [
        "## Question 3: Count records\n",
        "\n",
        "How many taxi trips were there on the 15th of October?\n",
        "\n",
        "Consider only trips that started on the 15th of October.\n",
        "\n",
        "- 85,567\n",
        "- 105,567\n",
        "- 125,567\n",
        "- 145,567"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvYT7K6dcoQz",
        "outputId": "ab087242-52bf-48a1-bf45-65ea788afb0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Taxi trips on October 15th: 128893\n"
          ]
        }
      ],
      "source": [
        "count = df.filter(F.to_date(df.tpep_pickup_datetime) == '2024-10-15').count()\n",
        "print(f\"Taxi trips on October 15th: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPya2Szqe7Ig",
        "outputId": "f641d630-51a8-457e-f9af-3c349a2c8c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Taxi trips on October 15th: 128893\n"
          ]
        }
      ],
      "source": [
        "# Use Spark SQL to count trips on October 15th\n",
        "count = spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT COUNT(*) AS trip_count\n",
        "    FROM trip_data\n",
        "    WHERE DATE(tpep_pickup_datetime) = '2024-10-15'\n",
        "    \"\"\"\n",
        ").collect()[0][0]\n",
        "\n",
        "print(f\"Taxi trips on October 15th: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3y0sdJ2dFM-"
      },
      "source": [
        "## Question 4: Longest trip\n",
        "\n",
        "What is the length of the longest trip in the dataset in hours?\n",
        "\n",
        "- 122\n",
        "- 142\n",
        "- 162\n",
        "- 182"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFSSeK1Wc1CD",
        "outputId": "898ba857-d7e4-44d2-80e7-d0d38ee8f242"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longest trip in hours: 162.61777777777777\n"
          ]
        }
      ],
      "source": [
        "# --- Question 4: Longest trip ---\n",
        "longest_trip_hours = df.agg(\n",
        "    F.max(\n",
        "        (F.unix_timestamp(F.col(\"tpep_dropoff_datetime\")) - F.unix_timestamp(F.col(\"tpep_pickup_datetime\"))) / 3600\n",
        "    )\n",
        ").collect()[0][0]\n",
        "print(f\"Longest trip in hours: {longest_trip_hours}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHeagCPIegGF",
        "outputId": "ad190975-4195-43c7-fb25-420525b173cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Longest trip in hours: 162.61777777777777\n"
          ]
        }
      ],
      "source": [
        "# Use Spark SQL to calculate the longest trip in hours\n",
        "longest_trip_hours = spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT MAX((UNIX_TIMESTAMP(tpep_dropoff_datetime) - UNIX_TIMESTAMP(tpep_pickup_datetime)) / 3600) AS longest_trip_hours\n",
        "    FROM trip_data\n",
        "    \"\"\"\n",
        ").collect()[0][0]\n",
        "\n",
        "print(f\"Longest trip in hours: {longest_trip_hours}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GroYwErfZ52"
      },
      "source": [
        "## Question 5: User Interface\n",
        "\n",
        "Spark’s User Interface which shows the application's dashboard runs on which local port?\n",
        "\n",
        "- 80\n",
        "- 443\n",
        "- 4040\n",
        "- 8080"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKLEHLXdfaYx",
        "outputId": "e7c68df7-9a88-4c7e-fcb0-35e748e644e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark UI Public URL: https://5e02-34-125-55-49.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# --- Question 5: User Interface ---\n",
        "ngrok_tunnel = ngrok.connect(4040)\n",
        "print(\"Spark UI Public URL:\", ngrok_tunnel.public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2Elesz1gOuo",
        "outputId": "d75a8819-9059-4bd4-812e-610196608427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark UI Port: 4040\n"
          ]
        }
      ],
      "source": [
        "spark_ui_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
        "print(f\"Spark UI Port: {spark_ui_port}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image](img/SparkUI_GoogleColab.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UFJSLa1dCuO"
      },
      "source": [
        "## Question 6: Least frequent pickup location zone\n",
        "\n",
        "Load the zone lookup data into a temp view in Spark:\n",
        "\n",
        "```bash\n",
        "wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
        "```\n",
        "\n",
        "Using the zone lookup data and the Yellow October 2024 data, what is the name of the LEAST frequent pickup location Zone?\n",
        "\n",
        "- Governor's Island/Ellis Island/Liberty Island\n",
        "- Arden Heights\n",
        "- Rikers Island\n",
        "- Jamaica Bay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO1ATfzshPE3",
        "outputId": "c508f4af-65db-4261-8574-f9caa1c0d14f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Least frequent pickup zone: Governor's Island/Ellis Island/Liberty Island\n"
          ]
        }
      ],
      "source": [
        "# --- Question 6: Least frequent pickup location zone ---\n",
        "# Using SQL\n",
        "# Load zone lookup data into a temporary view\n",
        "zone_df = spark.read.csv(\"taxi_zone_lookup.csv\", header=True)\n",
        "zone_df.createOrReplaceTempView(\"zone_lookup\")\n",
        "\n",
        "# Use Spark SQL to find the least frequent pickup zone\n",
        "least_frequent_zone = spark.sql(\n",
        "    \"\"\"\n",
        "    SELECT Zone, COUNT(*) AS pickup_count\n",
        "    FROM trip_data\n",
        "    JOIN zone_lookup ON trip_data.PULocationID = zone_lookup.LocationID\n",
        "    GROUP BY Zone\n",
        "    ORDER BY pickup_count ASC\n",
        "    LIMIT 1\n",
        "    \"\"\"\n",
        ").collect()[0][0]  # Extract the Zone value\n",
        "\n",
        "print(f\"Least frequent pickup zone: {least_frequent_zone}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUkv-y9YdSlJ",
        "outputId": "631854a9-7ad5-4234-8227-997e907fce22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Least frequent pickup zone: Governor's Island/Ellis Island/Liberty Island\n"
          ]
        }
      ],
      "source": [
        "# Using DataFrme\n",
        "zone_df = spark.read.csv(\"taxi_zone_lookup.csv\", header=True)\n",
        "df_with_zones = df.join(zone_df, df.PULocationID == zone_df.LocationID, \"left\")\n",
        "\n",
        "pickup_counts = df_with_zones.groupBy(\"Zone\").count()\n",
        "least_frequent_zone = pickup_counts.orderBy(\"count\").first().Zone\n",
        "print(f\"Least frequent pickup zone: {least_frequent_zone}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FGm7_5bi2lj",
        "outputId": "2c1c78e5-063f-4898-c651-a383aabd80e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-03-06T17:40:55+0000 lvl=warn msg=\"Stopping forwarder\" name=http-4040-a09e8bd3-45e2-457b-b3f4-6a19ee16b5eb acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        }
      ],
      "source": [
        "# Keep the tunnel alive (optional, but recommended)\n",
        "# import time\n",
        "# while True:\n",
        "#     time.sleep(60) # keep the tunnel alive for 1 minute.\n",
        "\n",
        "# Stop ngrok tunnel and SparkSession when finished (optional)\n",
        "ngrok.disconnect(ngrok_tunnel.public_url)\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Wpmq-ognEG"
      },
      "source": [
        "\n",
        "\n",
        "## Submitting the solutions\n",
        "\n",
        "- Form for submitting: https://courses.datatalks.club/de-zoomcamp-2025/homework/hw5\n",
        "- Deadline: See the website"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
