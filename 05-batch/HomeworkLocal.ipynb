{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5 Homework\n",
    "In this homework we'll put what we learned about Spark in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark[connect,pandas_on_spark,sql]\n",
      "  Downloading pyspark-3.5.5.tar.gz (317.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.2/317.2 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark[connect,pandas_on_spark,sql])\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /home/majeedk/anaconda3/lib/python3.11/site-packages (from pyspark[connect,pandas_on_spark,sql]) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=4.0.0 in /home/majeedk/anaconda3/lib/python3.11/site-packages (from pyspark[connect,pandas_on_spark,sql]) (14.0.2)\n",
      "Requirement already satisfied: numpy<2,>=1.15 in /home/majeedk/anaconda3/lib/python3.11/site-packages (from pyspark[connect,pandas_on_spark,sql]) (1.26.4)\n",
      "Requirement already satisfied: grpcio>=1.56.0 in /home/majeedk/anaconda3/lib/python3.11/site-packages (from pyspark[connect,pandas_on_spark,sql]) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status>=1.56.0 in /home/majeedk/anaconda3/lib/python3.11/site-packages (from pyspark[connect,pandas_on_spark,sql]) (1.70.0)\n",
      "Requirement already satisfied: googleapis-common-protos>=1.56.4 in /home/majeedk/anaconda3/lib/python3.11/site-packages (from pyspark[connect,pandas_on_spark,sql]) (1.66.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2 in /home/majeedk/anaconda3/lib/python3.11/site-packages (from googleapis-common-protos>=1.56.4->pyspark[connect,pandas_on_spark,sql]) (4.23.4)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2 (from googleapis-common-protos>=1.56.4->pyspark[connect,pandas_on_spark,sql])\n",
      "  Using cached protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/majeedk/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.5->pyspark[connect,pandas_on_spark,sql]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/majeedk/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.5->pyspark[connect,pandas_on_spark,sql]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/majeedk/anaconda3/lib/python3.11/site-packages (from pandas>=1.0.5->pyspark[connect,pandas_on_spark,sql]) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/majeedk/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->pyspark[connect,pandas_on_spark,sql]) (1.16.0)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Using cached protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.5-py2.py3-none-any.whl size=317747859 sha256=8c144292faf4523cc50b4505b6fef80c61518c8cebc2fa354697bb78b768e061\n",
      "  Stored in directory: /home/majeedk/.cache/pip/wheels/0c/7f/b4/0e68c6d8d89d2e582e5498ad88616c16d7c19028680e9d3840\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark, protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.23.4\n",
      "    Uninstalling protobuf-4.23.4:\n",
      "      Successfully uninstalled protobuf-4.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 5.29.3 which is incompatible.\n",
      "streamlit 1.29.0 requires importlib-metadata<7,>=1.4, but you have importlib-metadata 8.5.0 which is incompatible.\n",
      "streamlit 1.29.0 requires packaging<24,>=16.8, but you have packaging 24.2 which is incompatible.\n",
      "streamlit 1.29.0 requires protobuf<5,>=3.20, but you have protobuf 5.29.3 which is incompatible.\n",
      "streamlit 1.29.0 requires tenacity<9,>=8.1.0, but you have tenacity 9.0.0 which is incompatible.\n",
      "mlflow 2.9.2 requires importlib-metadata!=4.7.0,<8,>=3.7.0, but you have importlib-metadata 8.5.0 which is incompatible.\n",
      "mlflow 2.9.2 requires packaging<24, but you have packaging 24.2 which is incompatible.\n",
      "mlflow 2.9.2 requires protobuf<5,>=3.12.0, but you have protobuf 5.29.3 which is incompatible.\n",
      "mlflow 2.9.2 requires pytz<2024, but you have pytz 2024.1 which is incompatible.\n",
      "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
      "tensorflow 2.15.0 requires wrapt<1.15,>=1.11.0, but you have wrapt 1.17.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-5.29.3 py4j-0.10.9.7 pyspark-3.5.5\n"
     ]
    }
   ],
   "source": [
    "# Install PySpark and ngrok\n",
    "!pip install pyspark[sql,pandas_on_spark,connect]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start spark session\n",
    "spark = SparkSession.builder.appName(\"LocalSparkUI\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Install Spark and PySpark\n",
    "\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version.\n",
    "\n",
    "What's the output?\n",
    "\n",
    "> [!NOTE]\n",
    "> To install PySpark follow this [guide](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/pyspark.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.5\n"
     ]
    }
   ],
   "source": [
    "# --- Question 1: Install Spark and PySpark ---\n",
    "print(\"Spark Version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-06 18:47:25--  https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.239.238.133, 18.239.238.212, 18.239.238.152, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.239.238.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 64346071 (61M) [binary/octet-stream]\n",
      "Saving to: ‘data/yellow_tripdata_2024-10.parquet’\n",
      "\n",
      "yellow_tripdata_202 100%[===================>]  61.36M  62.5MB/s    in 1.0s    \n",
      "\n",
      "2025-03-06 18:47:26 (62.5 MB/s) - ‘data/yellow_tripdata_2024-10.parquet’ saved [64346071/64346071]\n",
      "\n",
      "--2025-03-06 18:47:27--  https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 18.239.238.152, 18.239.238.119, 18.239.238.212, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|18.239.238.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12331 (12K) [text/csv]\n",
      "Saving to: ‘data/taxi_zone_lookup.csv’\n",
      "\n",
      "taxi_zone_lookup.cs 100%[===================>]  12.04K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-03-06 18:47:27 (733 MB/s) - ‘data/taxi_zone_lookup.csv’ saved [12331/12331]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Download Data ---\n",
    "!mkdir -p data  # Create the 'data' directory if it doesn't exist\n",
    "!wget -P data https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
    "!wget -P data https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Yellow October 2024\n",
    "\n",
    "Read the October 2024 Yellow into a Spark Dataframe.\n",
    "\n",
    "Repartition the Dataframe to 4 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"data/yellow_tripdata_2024-10.parquet\")\n",
    "df_repartitioned = df.repartition(4)\n",
    "df_repartitioned.write.parquet(\"data/yellow_tripdata_2024-10-repartitioned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Parquet file size: 22.394158363342285 MB\n"
     ]
    }
   ],
   "source": [
    " # Calculate average file size\n",
    "parquet_files = [f for f in os.listdir(\"data/yellow_tripdata_2024-10-repartitioned.parquet\") if f.endswith(\".parquet\")]\n",
    "total_size = 0\n",
    "for file in parquet_files:\n",
    "    total_size += os.path.getsize(os.path.join(\"data/yellow_tripdata_2024-10-repartitioned.parquet\", file))\n",
    "average_size_mb = (total_size / len(parquet_files)) / (1024 * 1024)\n",
    "print(f\"Average Parquet file size: {average_size_mb} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Parquet file size: 22.394158363342285 MB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name, sum\n",
    "\n",
    "# Read Parquet files using binaryFiles\n",
    "file_sizes = spark.read.format(\"binaryFile\").load(\"data/yellow_tripdata_2024-10-repartitioned.parquet/*.parquet\") \\\n",
    "    .select(input_file_name().alias(\"path\"), \"length\")  # Select 'length' column directly\n",
    "\n",
    "# Calculate the total size and count of files\n",
    "total_size = file_sizes.agg(sum(\"length\")).collect()[0][0]\n",
    "num_files = file_sizes.count()\n",
    "\n",
    "# Calculate the average size in MB\n",
    "average_size_mb = (total_size / num_files) / (1024 * 1024)\n",
    "\n",
    "print(f\"Average Parquet file size: {average_size_mb} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Count records \n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Spark SQL\n",
    "# Register the DataFrame as a temporary view\n",
    "df.createOrReplaceTempView(\"trip_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi trips on October 15th: 128893\n"
     ]
    }
   ],
   "source": [
    "# Use Spark SQL to count trips on October 15th\n",
    "count = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(*) AS trip_count\n",
    "    FROM trip_data\n",
    "    WHERE DATE(tpep_pickup_datetime) = '2024-10-15'\n",
    "    \"\"\"\n",
    ").collect()[0][0]\n",
    "\n",
    "print(f\"Taxi trips on October 15th: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxi trips on October 15th: 128893\n"
     ]
    }
   ],
   "source": [
    "# Using data frame instead of SQL\n",
    "count = df.filter(F.to_date(df.tpep_pickup_datetime) == '2024-10-15').count()\n",
    "print(f\"Taxi trips on October 15th: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Longest trip\n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest trip in hours: 162.61777777777777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Use Spark SQL to calculate the longest trip in hours\n",
    "longest_trip_hours = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT MAX((UNIX_TIMESTAMP(tpep_dropoff_datetime) - UNIX_TIMESTAMP(tpep_pickup_datetime)) / 3600) AS longest_trip_hours\n",
    "    FROM trip_data\n",
    "    \"\"\"\n",
    ").collect()[0][0]\n",
    "\n",
    "print(f\"Longest trip in hours: {longest_trip_hours}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest trip in hours: 162.61777777777777\n"
     ]
    }
   ],
   "source": [
    "# Using Data Frame instead\n",
    "longest_trip_hours = df.agg(\n",
    "    F.max(\n",
    "        (F.unix_timestamp(F.col(\"tpep_dropoff_datetime\")) - F.unix_timestamp(F.col(\"tpep_pickup_datetime\"))) / 3600\n",
    "    )\n",
    ").collect()[0][0]\n",
    "print(f\"Longest trip in hours: {longest_trip_hours}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: User Interface\n",
    "\n",
    "Spark’s User Interface which shows the application's dashboard runs on which local port?\n",
    "\n",
    "![Image](img/SparkUI_Local.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Least frequent pickup location zone\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark:\n",
    "\n",
    "```bash\n",
    "wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
    "```\n",
    "\n",
    "Using the zone lookup data and the Yellow October 2024 data, what is the name of the LEAST frequent pickup location Zone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least frequent pickup zone: Governor's Island/Ellis Island/Liberty Island\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Using SQL\n",
    "# Load zone lookup data into a temporary view\n",
    "zone_df = spark.read.csv(\"data/taxi_zone_lookup.csv\", header=True)\n",
    "zone_df.createOrReplaceTempView(\"zone_lookup\")\n",
    "\n",
    "# Use Spark SQL to find the least frequent pickup zone\n",
    "least_frequent_zone = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT Zone, COUNT(*) AS pickup_count\n",
    "    FROM trip_data\n",
    "    JOIN zone_lookup ON trip_data.PULocationID = zone_lookup.LocationID\n",
    "    GROUP BY Zone\n",
    "    ORDER BY pickup_count ASC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    ").collect()[0][0]  # Extract the Zone value\n",
    "\n",
    "print(f\"Least frequent pickup zone: {least_frequent_zone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least frequent pickup zone: Governor's Island/Ellis Island/Liberty Island\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_with_zones = df.join(zone_df, df.PULocationID == zone_df.LocationID, \"left\")\n",
    "\n",
    "pickup_counts = df_with_zones.groupBy(\"Zone\").count()\n",
    "least_frequent_zone = pickup_counts.orderBy(\"count\").first().Zone\n",
    "print(f\"Least frequent pickup zone: {least_frequent_zone}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
